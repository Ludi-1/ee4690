{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import larq as lq\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def print_image(image):\n",
    "  # Squeeze the third dimension or you can use indexing to select the first slice\n",
    "  image_2d = np.squeeze(image)\n",
    "\n",
    "  # Plotting the image\n",
    "  plt.imshow(image_2d, cmap='gray')  # Use the gray colormap for grayscale\n",
    "  plt.colorbar()  # Optionally add a colorbar to see the intensity scale\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "\n",
    "print_image(test_images[0])\n",
    "# Normalize pixel values to be between -1 and 1\n",
    "train_images, test_images = train_images / 127.5 - 1, test_images / 127.5 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "938/938 [==============================] - 4s 3ms/step - loss: 1.2980 - accuracy: 0.8367\n",
      "Epoch 2/6\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.8004 - accuracy: 0.8928\n",
      "Epoch 3/6\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.6915 - accuracy: 0.9072\n",
      "Epoch 4/6\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.6387 - accuracy: 0.9158\n",
      "Epoch 5/6\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.5681 - accuracy: 0.9259\n",
      "Epoch 6/6\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.5487 - accuracy: 0.9308\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3d429cc9d0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NN Topology\n",
    "kwargs = dict(input_quantizer=\"ste_sign\",\n",
    "              kernel_quantizer=\"ste_sign\",\n",
    "              kernel_constraint=\"weight_clip\")\n",
    "input_shape = (28, 28, 1)\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape = input_shape))\n",
    "model.add(lq.layers.QuantDense(128*5, use_bias=False, **kwargs))\n",
    "model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "model.add(lq.layers.QuantDense(128*2, use_bias=False, **kwargs))\n",
    "model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "model.add(lq.layers.QuantDense(64, use_bias=False, **kwargs))\n",
    "model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "model.add(lq.layers.QuantDense(10, use_bias=False, **kwargs))\n",
    "model.add(tf.keras.layers.Activation(\"softmax\"))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, batch_size=64, epochs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 856us/step - loss: 0.8010 - accuracy: 0.9143\n",
      "+sequential_8 stats--------------------------------------------------------------------+\n",
      "| Layer                  Input prec.    Outputs  # 1-bit  # 32-bit  Memory  1-bit MACs |\n",
      "|                              (bit)                 x 1       x 1    (kB)             |\n",
      "+--------------------------------------------------------------------------------------+\n",
      "| flatten_8                        -  (-1, 784)        0         0       0           0 |\n",
      "| quant_dense_16                   1  (-1, 640)   501760         0   61.25      501760 |\n",
      "| batch_normalization_7            -  (-1, 640)        0      1280    5.00           0 |\n",
      "| quant_dense_17                   1  (-1, 256)   163840         0   20.00      163840 |\n",
      "| batch_normalization_8            -  (-1, 256)        0       512    2.00           0 |\n",
      "| quant_dense_18                   1   (-1, 64)    16384         0    2.00       16384 |\n",
      "| batch_normalization_9            -   (-1, 64)        0       128    0.50           0 |\n",
      "| quant_dense_19                   1   (-1, 10)      640         0    0.08         640 |\n",
      "| activation_8                     -   (-1, 10)        0         0       0           ? |\n",
      "+--------------------------------------------------------------------------------------+\n",
      "| Total                                           682624      1920   90.83      682624 |\n",
      "+--------------------------------------------------------------------------------------+\n",
      "+sequential_8 summary-------------------------+\n",
      "| Total params                      685 k     |\n",
      "| Trainable params                  683 k     |\n",
      "| Non-trainable params              1.92 k    |\n",
      "| Model size                        90.83 KiB |\n",
      "| Model size (8-bit FP weights)     85.20 KiB |\n",
      "| Float-32 Equivalent               2.61 MiB  |\n",
      "| Compression Ratio of Memory       0.03      |\n",
      "| Number of MACs                    683 k     |\n",
      "| Ratio of MACs that are binarized  1.0000    |\n",
      "+---------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Train NN\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "lq.models.summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "FC_TEMPLATE = \"\"\"module layer_%LAYER_NUM%_fc #(\n",
    "    parameter INPUT_DIM = %INPUT_DIM%,\n",
    "    parameter OUTPUT_DIM = %OUTPUT_DIM%\n",
    ") (\n",
    "    input [INPUT_DIM-1:0] i_data,\n",
    "    output reg signed [$clog2(INPUT_DIM):0] o_data [OUTPUT_DIM-1:0]\n",
    ");\n",
    "\n",
    "wire xnor_result [INPUT_DIM-1:0][OUTPUT_DIM-1:0];\n",
    "reg [$clog2(INPUT_DIM):0] popcnt [OUTPUT_DIM-1:0];\n",
    "reg [$clog2(INPUT_DIM):0] shift [OUTPUT_DIM-1:0];\n",
    "\n",
    "%XNOR_GEN%\n",
    "\n",
    "localparam CONCAT_BITS = $clog2(INPUT_DIM)-1;\n",
    "\n",
    "always_comb begin\n",
    "    for (int i = 0; i < OUTPUT_DIM; i++) begin\n",
    "        popcnt[i] = 0;\n",
    "        for (int j = 0; j < INPUT_DIM; j++) begin\n",
    "            popcnt[i] += { {CONCAT_BITS{1'b0}}, xnor_result[j][i]};\n",
    "        end\n",
    "        shift[i] = popcnt[i] << 1;\n",
    "        o_data[i] = shift[i] - INPUT_DIM;\n",
    "    end\n",
    "end\n",
    "\n",
    "endmodule\"\"\"\n",
    "\n",
    "BN_TEMPLATE = \"\"\"module layer_%LAYER_NUM%_bn #(\n",
    "    parameter INPUT_DIM = 1,\n",
    "    parameter OUTPUT_DIM = 1\n",
    ")(\n",
    "    input signed [$clog2(INPUT_DIM):0] i_data [OUTPUT_DIM-1:0],\n",
    "    output [OUTPUT_DIM-1:0] o_data\n",
    ");\n",
    "\n",
    "%COMPARE%\n",
    "\n",
    "endmodule\n",
    "\"\"\"\n",
    "\n",
    "TOP_TEMPLATE = \"\"\"module top #(\n",
    "    parameter IMG_DIM = 28,\n",
    "%PARAMETERS%\n",
    ") (\n",
    "%PORTS%\n",
    "    input clk,\n",
    "    input i_we,\n",
    "    input [$clog2(L0_INPUT_DIM)-1:0] i_addr,\n",
    "    input i_data\n",
    ");\n",
    "\n",
    "%SIGNALS%\n",
    "\n",
    "ibuf #(\n",
    "    .IMG_DIM(IMG_DIM)\n",
    ") ibuf1 (\n",
    "    .clk(clk),\n",
    "    .i_we(i_we),\n",
    "    .i_addr(i_addr),\n",
    "    .i_data(i_data),\n",
    "    .o_data(L0_i_data)\n",
    ");\n",
    "\n",
    "%MODULES%\n",
    "\n",
    "endmodule\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "if not os.path.exists(\"gen_hdl\"):\n",
    "    os.mkdir(\"gen_hdl\")\n",
    "else:\n",
    "    shutil.rmtree('gen_hdl')\n",
    "    os.mkdir(\"gen_hdl\")\n",
    "\n",
    "def parse_fc(fc_weights, num: int):\n",
    "    fc_weights[fc_weights == -1] = 0\n",
    "    xnor = \"\"\n",
    "    weight_dim = fc_weights.shape\n",
    "    for input_neuron in range(weight_dim[0]):\n",
    "        for output_neuron in range(weight_dim[1]):\n",
    "            weight = fc_weights[input_neuron][output_neuron]\n",
    "            if weight == 0:\n",
    "                xnor += f\"assign xnor_result[{input_neuron}][{output_neuron}] = ~i_data[{input_neuron}];\\n\"\n",
    "            elif weight == 1:\n",
    "                xnor += f\"assign xnor_result[{input_neuron}][{output_neuron}] = i_data[{input_neuron}];\\n\"\n",
    "            else:\n",
    "                raise Exception(f\"neuron value not 0 or 1: {input_neuron}\")\n",
    "\n",
    "    output_hdl = FC_TEMPLATE \\\n",
    "        .replace(\"%XNOR_GEN%\", xnor) \\\n",
    "        .replace(\"%LAYER_NUM%\", str(num)) \\\n",
    "        .replace(\"%INPUT_DIM%\", str(weight_dim[0])) \\\n",
    "        .replace(\"%OUTPUT_DIM%\", str(weight_dim[1]))\n",
    "    with open(f\"gen_hdl/L{num}_fc.v\", \"w\") as f:\n",
    "        f.write(output_hdl)\n",
    "    return (weight_dim[0], weight_dim[1])\n",
    "\n",
    "def parse_bn(beta, moving_mean, moving_variance, num: int):\n",
    "\n",
    "    # thresholds = np.zeros(len(beta))\n",
    "    print(beta.shape)\n",
    "    compare = \"\"\n",
    "    for output_neuron in range(len(beta)):\n",
    "        # print(len(beta))\n",
    "        threshold = moving_mean[output_neuron] - beta[output_neuron] * np.sqrt(moving_variance[output_neuron])\n",
    "        compare += f\"\\tassign o_data[{output_neuron}] = i_data[{output_neuron}] > {round(threshold)} ? 1 : 0;\\n\"\n",
    "\n",
    "    output_hdl = BN_TEMPLATE \\\n",
    "        .replace(\"%DIM_DATA%\", str(len(beta))) \\\n",
    "        .replace(\"%LAYER_NUM%\", str(num)) \\\n",
    "        .replace(\"%COMPARE%\", compare)\n",
    "        \n",
    "    with open(f\"gen_hdl/L{num}_bn.v\", \"w\") as f:\n",
    "        f.write(output_hdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "(640,)\n",
      "(256,)\n",
      "(64,)\n"
     ]
    }
   ],
   "source": [
    "parameters = \"\"\n",
    "signals = \"\"\n",
    "modules = \"\"\n",
    "ports = \"\"\n",
    "print(len(model.layers)-2)\n",
    "n = 0\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "        beta, moving_mean, moving_variance = layer.get_weights()\n",
    "        parse_bn(beta, moving_mean, moving_variance, n)\n",
    "        signals += (\n",
    "            f'wire [$clog2(L{n-1}_INPUT_DIM):0] L{n}_i_data [L{n-1}_OUTPUT_DIM-1:0];\\n'\n",
    "        )\n",
    "        modules += (\n",
    "            f'layer_{n}_bn #(\\n'\n",
    "            f'\\t.INPUT_DIM(L{n-1}_INPUT_DIM),\\n'\n",
    "            f'\\t.OUTPUT_DIM(L{n-1}_OUTPUT_DIM)\\n'\n",
    "            f') L{n}_bn (\\n'\n",
    "            f'\\t.i_data(L{n}_i_data),\\n'\n",
    "            f'\\t.o_data(L{n+1}_i_data)\\n);\\n'\n",
    "        )\n",
    "        n += 1\n",
    "    elif isinstance(layer, lq.layers.QuantDense):\n",
    "        with lq.context.quantized_scope(True):\n",
    "            if n == len(model.layers)-3:\n",
    "                classifier = 1\n",
    "            else:\n",
    "                classifier = 1\n",
    "            weights = layer.get_weights()\n",
    "            input_dim, output_dim = parse_fc(weights[0], n)\n",
    "            parameters += (\n",
    "                f'\\tparameter L{n}_INPUT_DIM = {input_dim},\\n'\n",
    "                f'\\tparameter L{n}_OUTPUT_DIM = {output_dim},\\n'\n",
    "                )\n",
    "            signals += (\n",
    "                f'wire [L{n}_INPUT_DIM-1:0] L{n}_i_data;\\n'\n",
    "            )\n",
    "            modules += (\n",
    "                f'layer_{n}_fc #(\\n'\n",
    "                f'\\t.INPUT_DIM(L{n}_INPUT_DIM),\\n'\n",
    "                f'\\t.OUTPUT_DIM(L{n}_OUTPUT_DIM)\\n'\n",
    "                f') L{n}_fc (\\n'\n",
    "                f'\\t.i_data(L{n}_i_data),\\n'\n",
    "                f'\\t.o_data(L{n+1}_i_data)\\n);\\n'\n",
    "            )\n",
    "            n += 1\n",
    "\n",
    "signals += (\n",
    "    f'wire [$clog2(L{n-1}_INPUT_DIM):0] L{n}_i_data [L{n-1}_OUTPUT_DIM-1:0];\\n'\n",
    "    f'assign o_data = L{n}_i_data;\\n'\n",
    ")\n",
    "ports += (\n",
    "    f'\\toutput signed [$clog2(L{n-1}_INPUT_DIM):0] o_data [9:0],\\n'\n",
    ")\n",
    "\n",
    "output_hdl = TOP_TEMPLATE \\\n",
    "        .replace(\"%PARAMETERS%\", parameters.rstrip(\",\\n\")) \\\n",
    "        .replace(\"%PORTS%\", ports) \\\n",
    "        .replace(\"%SIGNALS%\", signals) \\\n",
    "        .replace(\"%MODULES%\", modules)\n",
    "\n",
    "with open(f\"./gen_hdl/top.v\", \"w\") as f:\n",
    "    f.write(output_hdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
