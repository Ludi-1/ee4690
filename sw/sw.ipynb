{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import larq as lq\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.keras import initializers, regularizers, constraints\n",
    "from tensorflow.python.keras.utils import tf_utils\n",
    "from tensorflow.python.ops import standard_ops\n",
    "\n",
    "import templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def print_image(image):\n",
    "  # Squeeze the third dimension or you can use indexing to select the first slice\n",
    "  image_2d = np.squeeze(image)\n",
    "\n",
    "  # Plotting the image\n",
    "  plt.imshow(image_2d, cmap='gray')  # Use the gray colormap for grayscale\n",
    "  plt.colorbar()  # Optionally add a colorbar to see the intensity scale\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "\n",
    "print_image(train_images[0])\n",
    "# Normalize pixel values to be between -1 and 1\n",
    "train_images, test_images = train_images / 127.5 - 1, test_images / 127.5 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputShapeCallback(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.input_shapes = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \n",
    "        for layer in self.model.layers:\n",
    "            if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "                input_shape = layer.input_shape\n",
    "                self.input_shapes.append((layer.name, input_shape))\n",
    "                print(f\"Layer: {layer.name}, Input shape: {input_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heights: [25, 12, 12, 10, 5, 5, None, None, None, None, None, None]\n",
      "Widths: [25, 12, 12, 10, 5, 5, None, None, None, None, None, None]\n",
      "Channels: [32, 32, 32, 32, 32, 32, 800, 128, 128, 10, 10, 10]\n"
     ]
    }
   ],
   "source": [
    "# NN Topology\n",
    "\n",
    "kwargs = dict(input_quantizer=\"ste_sign\",\n",
    "              kernel_quantizer=\"ste_sign\",\n",
    "              kernel_constraint=\"weight_clip\")\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "input_shape = (28, 28, 1) # Input img shape\n",
    "filters_a = 32 # Number of output channels\n",
    "kernel_three = (4, 4) # Kernel dimension\n",
    "\n",
    "filters_b = 32 # Number of output channels\n",
    "kernel_b = (3, 3) # Kernel dimension\n",
    "\n",
    "model.add(lq.layers.QuantConv2D(filters_a, kernel_three,\n",
    "                                input_quantizer=\"ste_sign\",\n",
    "                                kernel_quantizer=\"ste_sign\",\n",
    "                                kernel_constraint=\"weight_clip\",\n",
    "                                use_bias=False,\n",
    "                                input_shape=input_shape))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "model.add(lq.layers.QuantConv2D(filters_b, kernel_b,\n",
    "                                input_quantizer=\"ste_sign\",\n",
    "                                kernel_quantizer=\"ste_sign\",\n",
    "                                kernel_constraint=\"weight_clip\",\n",
    "                                use_bias=False,\n",
    "                                input_shape=input_shape))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(lq.layers.QuantDense(128, use_bias=False, **kwargs))\n",
    "model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "model.add(lq.layers.QuantDense(10, use_bias=False, **kwargs))\n",
    "model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "model.add(tf.keras.layers.Activation(\"softmax\"))\n",
    "# model.add(tf.keras.layers.Flatten())\n",
    "# # model.add(lq.layers.QuantDense(500, use_bias=False, **kwargs))\n",
    "# model.add(lq.layers.QuantDense(10, use_bias=False, **kwargs))\n",
    "# model.add(tf.keras.layers.Activation(\"softmax\"))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "output_shapes = [layer.output_shape for layer in model.layers]\n",
    "\n",
    "heights = []\n",
    "widths = []\n",
    "channels = []\n",
    "\n",
    "for shape in output_shapes:\n",
    "    if len(shape) == 4:  \n",
    "        _, height, width, channel = shape\n",
    "        heights.append(height)\n",
    "        widths.append(width)\n",
    "        channels.append(channel)\n",
    "    elif len(shape) == 2:  \n",
    "        _, channel = shape\n",
    "        heights.append(None)\n",
    "        widths.append(None)\n",
    "        channels.append(channel)\n",
    "\n",
    "print(\"Heights:\", heights)\n",
    "print(\"Widths:\", widths)\n",
    "print(\"Channels:\", channels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "938/938 [==============================] - 17s 17ms/step - loss: 0.6584 - accuracy: 0.9077\n",
      "Epoch 2/6\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.5037 - accuracy: 0.9560\n",
      "Epoch 3/6\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.4769 - accuracy: 0.9639\n",
      "Epoch 4/6\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.4681 - accuracy: 0.9667\n",
      "Epoch 5/6\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.4615 - accuracy: 0.9682\n",
      "Epoch 6/6\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.4552 - accuracy: 0.9701\n",
      "Conv layer 1 weights shape: [(4, 4, 1, 32)]\n",
      "Conv layer 2 weights shape: [(3, 3, 32, 32)]\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, batch_size=64, epochs=6)\n",
    "\n",
    "conv_weights = []\n",
    "\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, lq.layers.QuantConv2D):\n",
    "        weights = layer.get_weights()  \n",
    "        conv_weights.append(weights)\n",
    "\n",
    "\n",
    "for idx, weights in enumerate(conv_weights):\n",
    "    print(f\"Conv layer {idx + 1} weights shape: {[w.shape for w in weights]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[-3.05359185e-01  1.22815840e-01  3.42647225e-01  1.28391501e-03\n",
      "    -1.30928218e-01  4.05715294e-02 -1.87652535e-03  1.37864926e-03\n",
      "     3.39959383e-01  6.48676008e-02 -4.93577728e-03  1.11516844e-03\n",
      "     3.35706212e-03  4.45731133e-02 -1.95452198e-01  1.57760799e-01\n",
      "    -2.93332696e-01 -7.71552995e-02 -8.45448859e-03  1.69905939e-03\n",
      "    -3.53180707e-01  7.23499246e-03 -4.38118204e-02  1.31393999e-01\n",
      "     5.21681039e-04  3.39547247e-01  1.96271925e-03  3.50143760e-01\n",
      "    -1.07463583e-01 -3.98404838e-04 -4.60931100e-03 -9.33076080e-04]]\n",
      "\n",
      "  [[-2.86079347e-01  2.52935648e-01 -8.98610335e-03  9.64165945e-03\n",
      "     6.43169682e-04  2.02691734e-01 -1.75698008e-03  1.64705198e-02\n",
      "     5.82107425e-01 -1.56878144e-03 -1.47467911e-01  8.79047886e-02\n",
      "    -2.67115403e-02 -3.45480703e-02 -1.74883649e-01 -7.48347640e-02\n",
      "    -8.39414299e-02 -1.30252719e-01 -1.61811396e-01  2.96775788e-01\n",
      "    -1.83414738e-03 -3.94617915e-01  4.55022044e-03  1.04392104e-01\n",
      "     2.80845344e-01  1.78821340e-01 -2.54411879e-03  3.58678810e-02\n",
      "    -2.41598949e-01 -5.40152611e-03 -2.23549053e-01  6.70805424e-02]]\n",
      "\n",
      "  [[-1.62699237e-01  3.17187697e-01 -5.83116747e-02 -7.02929217e-04\n",
      "     8.70409831e-02  1.63738430e-01  1.99795246e-01  1.31678477e-01\n",
      "     4.22969371e-01 -9.52088982e-02 -3.66925728e-03  1.98254749e-01\n",
      "    -5.95709449e-03 -4.69653914e-03 -1.98179856e-01  6.93041511e-06\n",
      "    -7.85269886e-02  5.92402881e-03 -2.25546196e-01  1.97757900e-01\n",
      "    -2.87284935e-03 -5.79876006e-01 -1.86880713e-03 -1.89241201e-01\n",
      "     3.83887559e-01  2.08402006e-03 -7.33473003e-02 -2.45174998e-03\n",
      "    -2.15726286e-01 -4.52587605e-01 -3.22888762e-01  4.87417774e-03]]\n",
      "\n",
      "  [[ 1.90962657e-01 -5.55392262e-03 -1.21293120e-01  1.85957596e-01\n",
      "    -2.20623955e-01  1.64882988e-02  2.25898221e-01  6.88898144e-05\n",
      "    -6.08413806e-03 -3.87982905e-01 -2.73062617e-01  7.59326527e-03\n",
      "     3.85236472e-01  2.12074339e-01 -4.55496639e-01  1.98485702e-02\n",
      "    -1.09546341e-01  4.78104383e-01 -5.34546554e-01  1.36219352e-01\n",
      "     1.05523169e-01 -2.63433427e-01 -1.32743463e-01 -2.71880597e-01\n",
      "     1.66640311e-01  6.65204972e-03 -4.59833562e-01 -1.41471490e-01\n",
      "    -4.26101863e-01 -5.40673375e-01  2.29804683e-02 -3.28430533e-01]]]\n",
      "\n",
      "\n",
      " [[[-3.38627279e-01  3.09541523e-02  5.53786103e-03  7.08192885e-02\n",
      "     2.81886645e-02  1.32390335e-01 -1.85727939e-01  5.13574071e-02\n",
      "     1.00228384e-01  6.94191232e-02 -2.64727056e-01 -2.39978492e-01\n",
      "     4.24275398e-01  2.70065874e-01 -1.51416421e-01  2.43037507e-01\n",
      "    -2.70196557e-01 -1.52216491e-03 -6.44338354e-02  5.19779027e-02\n",
      "    -7.13894188e-01 -3.36491436e-01  2.42414951e-01  3.35028887e-01\n",
      "     6.54881005e-04  2.69226253e-01  1.30530670e-01  3.45738858e-01\n",
      "    -2.47668907e-01 -5.26475012e-01 -1.84172049e-01  2.85714623e-02]]\n",
      "\n",
      "  [[ 1.01361647e-01 -1.59521005e-03 -3.23955745e-01  1.11720175e-01\n",
      "     2.63293623e-03  1.14566681e-03 -1.58415928e-01  2.28515580e-01\n",
      "    -5.51780201e-02 -1.06640749e-01 -2.44072706e-01 -4.56333049e-02\n",
      "     1.25787361e-02  5.23034250e-03  5.47883520e-03 -1.87995449e-01\n",
      "     2.72486597e-01  2.43969128e-01 -2.97112763e-01  2.35315078e-04\n",
      "    -1.00000000e+00 -7.50681102e-01  2.66778409e-01  7.60067767e-03\n",
      "     8.38201642e-02 -3.56974192e-02  1.02587745e-01  2.33489461e-02\n",
      "    -3.90134841e-01 -9.89480853e-01  1.27791967e-02 -9.30733586e-05]]\n",
      "\n",
      "  [[ 1.50284007e-01  2.70782769e-01 -8.03751722e-02  7.13942200e-02\n",
      "     2.61574358e-01 -1.60909025e-03 -2.25503296e-02  3.24538797e-01\n",
      "    -3.37692708e-01 -3.24494392e-01 -2.13293992e-02 -1.05928443e-02\n",
      "    -1.34724183e-02 -2.22887054e-01  1.57057270e-01 -2.81868335e-02\n",
      "     1.61114529e-01  1.77580759e-01 -3.38254064e-01  2.16065440e-03\n",
      "    -9.05140519e-01 -3.43276173e-01 -1.14863247e-01 -1.81354225e-01\n",
      "    -1.92794704e-03 -5.48903011e-02 -3.69859397e-01  3.19612678e-03\n",
      "    -3.25644732e-01 -7.77092457e-01 -3.60285759e-01 -5.08600295e-01]]\n",
      "\n",
      "  [[-1.09661801e-03 -1.21267080e-01  5.42572662e-02  2.42667988e-01\n",
      "     1.07419109e-02 -7.96620622e-02  7.67597975e-03 -5.80931678e-02\n",
      "    -2.92407155e-01 -2.84522563e-01  3.61422487e-02  4.33734022e-02\n",
      "     7.10356593e-01  9.24124470e-05 -9.26039815e-02  1.27828509e-01\n",
      "    -2.20258418e-03  5.19107461e-01 -1.97898507e-01 -3.74359310e-01\n",
      "    -1.04891425e-02  9.34830983e-04 -2.46747985e-01  1.49767650e-02\n",
      "    -2.54303426e-01  4.57754791e-01 -5.27671456e-01  1.48877325e-02\n",
      "    -3.54573518e-01 -4.19791937e-01 -3.76650631e-01 -1.00000000e+00]]]\n",
      "\n",
      "\n",
      " [[[-7.10502118e-02  3.79225649e-02 -9.02164802e-02  1.82231143e-03\n",
      "     2.71078318e-01  4.24820231e-04 -4.98602390e-01 -1.51389523e-03\n",
      "     2.12827608e-01  1.63767800e-01  4.87200245e-02 -2.06931785e-01\n",
      "     5.74632347e-01  3.63530964e-01 -2.47324686e-02 -2.63386041e-01\n",
      "     1.59209818e-01 -2.02128431e-03  1.89143419e-02 -4.45527315e-01\n",
      "    -2.15213140e-03 -7.35236764e-01  1.60256237e-01  6.29043207e-02\n",
      "    -4.75776847e-03  1.15734739e-02  6.05377927e-02  1.89054564e-01\n",
      "     4.10849787e-02 -2.74065673e-01  4.98566683e-03  6.06161123e-03]]\n",
      "\n",
      "  [[ 1.11642726e-01 -2.25615827e-03 -3.24969798e-01 -1.31220967e-02\n",
      "     2.73813039e-01 -1.37403995e-01 -7.94258595e-01  2.15329170e-01\n",
      "     1.16294242e-01 -2.23523169e-03 -6.97109029e-02  2.19132960e-01\n",
      "     2.80249447e-01  1.62185654e-01  3.02702218e-01 -6.10220134e-01\n",
      "     5.11479616e-01 -1.48314431e-01 -1.46649987e-01 -6.57016039e-01\n",
      "    -8.34115446e-01 -3.03569168e-01  5.68157993e-02 -2.91345431e-03\n",
      "     2.21740171e-01 -3.41718122e-02  1.55771688e-01 -8.69431198e-02\n",
      "     2.80609645e-04 -7.69461244e-02  2.87538975e-01 -1.63683668e-01]]\n",
      "\n",
      "  [[ 1.74444929e-01  1.03948087e-01 -2.44406611e-02 -2.48580083e-01\n",
      "     4.66443628e-01 -5.09076715e-01 -5.06391346e-01  1.54990107e-01\n",
      "    -9.49686468e-02 -2.01542571e-01  3.29363167e-01  4.13044184e-01\n",
      "    -1.61883995e-01 -2.62883574e-01  1.03006676e-01 -6.71734989e-01\n",
      "     8.56593833e-05 -8.60666931e-02 -2.82294513e-03 -8.42565894e-01\n",
      "    -9.98024940e-01 -1.03880325e-02 -3.82467359e-01  5.63570112e-02\n",
      "     1.38826584e-02  2.36922577e-01 -2.44509429e-01  1.33494288e-01\n",
      "    -1.51703577e-03 -1.06477970e-03  5.73707919e-04 -8.58269870e-01]]\n",
      "\n",
      "  [[-2.91741937e-01 -3.79064381e-01  3.53015453e-01  1.12487748e-02\n",
      "     1.75240159e-01 -4.53369707e-01 -2.72097141e-01 -2.94987768e-01\n",
      "    -5.10224700e-03 -1.84132457e-02  1.78749502e-01  3.76657665e-01\n",
      "     3.20665479e-01 -3.30109417e-01  4.20032209e-03 -1.27619077e-02\n",
      "     3.76524121e-01  2.00179636e-01 -1.14367227e-04 -7.63830304e-01\n",
      "    -5.75928390e-01  1.93863064e-02 -9.86059830e-02  8.97475123e-01\n",
      "    -7.71084940e-03  7.94000387e-01 -1.88332558e-01  2.94404209e-01\n",
      "     9.07951146e-02  4.92012054e-02 -9.20784622e-02 -8.17384541e-01]]]\n",
      "\n",
      "\n",
      " [[[ 1.29516453e-01 -1.32943973e-01 -3.49529386e-01 -4.47015673e-01\n",
      "     7.24995462e-03 -1.26243412e-01  5.78854531e-02 -2.39570513e-01\n",
      "     1.79920532e-03  6.26750290e-02  3.28471571e-01 -3.60196531e-01\n",
      "     5.00893116e-01  3.49418551e-01  1.03195034e-01  2.72991478e-01\n",
      "    -2.14690901e-03 -6.90002274e-03  4.66392227e-02 -9.60298717e-01\n",
      "     2.68895239e-01 -8.17350328e-01 -4.97594848e-02  1.12181425e-03\n",
      "     8.48081410e-01  1.63848430e-01  2.32986003e-01  1.00826391e-03\n",
      "     4.30672280e-02 -1.53188379e-02  3.59491825e-01 -3.30931209e-02]]\n",
      "\n",
      "  [[ 1.99928507e-03  2.25021467e-01 -2.50855923e-01 -5.08812904e-01\n",
      "    -7.22661614e-03 -6.72664165e-01 -3.97350416e-02 -1.85442135e-01\n",
      "     1.91538423e-01  1.13017179e-01  1.59621462e-01 -1.62492245e-02\n",
      "    -7.85503536e-03 -1.21321626e-01  5.85916154e-02 -6.02995940e-02\n",
      "     2.65889764e-01  2.58298188e-01  5.15322061e-03 -8.80819082e-01\n",
      "     4.57145274e-02 -1.70659527e-01 -2.86696821e-01 -1.83911063e-03\n",
      "     7.16172218e-01  9.41227153e-02  2.40770757e-01  1.28215901e-03\n",
      "     1.22725517e-02  4.05163690e-02  4.12005603e-01 -5.74620068e-01]]\n",
      "\n",
      "  [[-3.19547877e-02 -5.71134016e-02  5.93196601e-02 -4.18914497e-01\n",
      "    -1.06188357e-01 -6.01293683e-01 -3.17757204e-02 -8.97848085e-02\n",
      "     3.82258236e-01  8.78715739e-02  1.84722349e-01  1.06888160e-01\n",
      "    -4.64960411e-02 -2.58534312e-01  1.92023695e-01 -1.34478912e-01\n",
      "    -3.12926173e-02  7.09364891e-01  1.04203902e-01 -2.16045395e-01\n",
      "     7.90819153e-03 -5.26086078e-04 -7.25660706e-03  1.90980613e-01\n",
      "     2.22808301e-01  1.14893839e-01  1.29265025e-01  1.55372351e-01\n",
      "     5.56701049e-02  2.09082104e-02  2.90121317e-01 -9.01892006e-01]]\n",
      "\n",
      "  [[-3.88531715e-01 -2.03855887e-01  1.90734724e-03 -5.98292947e-01\n",
      "    -3.93750519e-01 -3.03634182e-02 -1.08114228e-01 -2.02140138e-01\n",
      "     3.32145512e-01  1.89144284e-01  2.07514018e-01 -7.16564357e-02\n",
      "    -6.98056258e-03 -2.13357657e-01  8.96413103e-02 -2.13243321e-01\n",
      "     1.45532921e-01  5.26318669e-01  1.62911639e-01 -7.13021785e-04\n",
      "    -2.22444668e-01  3.26728493e-01  3.86686802e-01  5.13308823e-01\n",
      "    -6.20924234e-02  8.99387896e-02 -6.13443670e-04  3.79042923e-01\n",
      "     3.30246845e-03  1.38906464e-02  2.89222982e-04 -1.40846558e-02]]]]\n",
      "[[[[-1.10614356e-02 -1.60930064e-02 -4.98340428e-02 ... -1.41462684e-03\n",
      "     4.07839194e-02 -1.33221179e-01]\n",
      "   [-3.61022563e-03  6.24806480e-03  6.71981052e-02 ... -3.57669182e-02\n",
      "    -2.46178489e-02 -1.74536690e-01]\n",
      "   [ 2.35347301e-01  3.06862369e-02 -8.01048940e-04 ... -3.93589363e-02\n",
      "     5.47624491e-02 -2.08645210e-01]\n",
      "   ...\n",
      "   [-1.49359897e-04  1.53962905e-02 -1.46697266e-02 ... -4.61958349e-02\n",
      "    -1.20521558e-03  7.48953363e-03]\n",
      "   [-1.02088422e-01 -3.07434760e-02  2.74426490e-03 ...  2.83336639e-01\n",
      "    -1.36834174e-01  5.25223464e-02]\n",
      "   [ 9.34970472e-03 -2.48439959e-03  3.62046361e-02 ... -1.79134794e-02\n",
      "     4.30466086e-02  6.32145554e-02]]\n",
      "\n",
      "  [[-7.12629259e-02  2.89053097e-02 -8.38156417e-02 ... -1.39943957e-01\n",
      "     5.40913921e-03 -1.73309557e-02]\n",
      "   [-2.51431465e-02 -5.24374917e-02 -2.65691727e-02 ...  4.15850431e-02\n",
      "    -6.51954301e-03 -1.16815162e-03]\n",
      "   [ 2.75151059e-02 -2.61897501e-03 -6.97461702e-03 ...  4.63542603e-02\n",
      "     1.59807518e-01 -1.03827715e-01]\n",
      "   ...\n",
      "   [ 1.81134865e-02  1.48931816e-01  1.55395418e-02 ...  8.70521273e-03\n",
      "     1.09071143e-01 -2.30310466e-02]\n",
      "   [-1.91793405e-03 -2.45104767e-02  2.83617619e-03 ...  3.85405570e-02\n",
      "    -2.58245692e-02 -1.23610057e-01]\n",
      "   [-2.86633745e-02  1.63499713e-02 -8.77354145e-02 ... -8.78591239e-02\n",
      "     6.45522494e-03  8.19291100e-02]]\n",
      "\n",
      "  [[-1.00276368e-02  3.54883559e-02 -1.49473117e-03 ...  6.86009601e-02\n",
      "    -7.27037117e-02  2.07308531e-02]\n",
      "   [ 2.47284416e-02 -1.26216143e-01 -4.49207425e-02 ...  5.22014452e-03\n",
      "    -4.03441116e-02  2.94024982e-02]\n",
      "   [ 6.62387684e-02  3.96037370e-01  4.66460809e-02 ...  3.38137858e-02\n",
      "    -5.86619824e-02 -3.49196136e-01]\n",
      "   ...\n",
      "   [-1.33020878e-02  1.11153172e-02  6.42967746e-02 ...  1.08454749e-02\n",
      "     8.77488777e-02  8.77137706e-02]\n",
      "   [ 4.11661007e-02 -4.53292057e-02  1.18228883e-01 ... -1.09323584e-01\n",
      "    -1.69151023e-01 -1.60343334e-01]\n",
      "   [-8.80731791e-02 -2.04375386e-02 -3.46606225e-01 ... -8.52588937e-02\n",
      "     1.17725976e-01  3.50903392e-01]]]\n",
      "\n",
      "\n",
      " [[[ 4.90096174e-02 -3.47032636e-01 -1.05725981e-01 ... -1.01997145e-01\n",
      "     2.91747187e-04  8.36008564e-02]\n",
      "   [-1.88455373e-01 -1.65130254e-02  7.47133791e-02 ...  1.41633004e-01\n",
      "    -5.28375767e-02 -3.26422840e-01]\n",
      "   [-3.14495921e-01  1.45784765e-01 -5.97337522e-02 ...  9.85157210e-04\n",
      "     1.57055892e-02  2.95737777e-02]\n",
      "   ...\n",
      "   [-7.31547177e-02  1.05725028e-01 -1.02404365e-02 ... -2.35552594e-01\n",
      "     2.28759274e-01  7.32360706e-02]\n",
      "   [-1.40873417e-01 -2.33918622e-01 -1.89703181e-01 ... -5.16760573e-02\n",
      "    -8.02549869e-02  1.52211459e-02]\n",
      "   [-2.51941895e-03  2.73970272e-02 -6.29966035e-02 ... -1.22128678e-02\n",
      "     1.07860669e-01 -1.67609289e-01]]\n",
      "\n",
      "  [[ 8.54417158e-05  4.67214733e-02  1.33271039e-01 ...  4.28319871e-02\n",
      "    -7.61972601e-03  3.62944230e-02]\n",
      "   [-8.95564035e-02 -3.32466125e-01  1.96726277e-01 ...  1.65035039e-01\n",
      "    -4.52451082e-03  2.93777767e-03]\n",
      "   [-1.96037993e-01  1.37337238e-01 -2.25584850e-01 ... -8.72945860e-02\n",
      "     1.59270197e-01 -2.63183564e-01]\n",
      "   ...\n",
      "   [-3.39885801e-02 -3.07622887e-02 -1.56107113e-01 ... -2.10008351e-03\n",
      "     2.03143492e-01  3.13860625e-02]\n",
      "   [-1.93809301e-01 -7.12412456e-03 -1.47318486e-02 ... -2.10722819e-01\n",
      "    -5.98408580e-02  1.23489732e-02]\n",
      "   [-5.01984078e-03  2.22018287e-01  5.24031976e-03 ...  5.66545576e-02\n",
      "     1.20924916e-02 -2.19599068e-01]]\n",
      "\n",
      "  [[ 9.05356952e-04  1.34432971e-01  3.48930776e-01 ...  2.32776292e-02\n",
      "    -5.53373247e-02 -1.82571579e-02]\n",
      "   [-1.20718934e-01 -9.57037974e-03 -1.50870811e-02 ...  3.95487845e-02\n",
      "    -1.17503360e-01 -4.24532071e-02]\n",
      "   [ 2.32063849e-02  3.24079722e-01 -2.48308495e-01 ...  4.62376571e-04\n",
      "     2.76022162e-02 -2.69732535e-01]\n",
      "   ...\n",
      "   [ 2.98066251e-03  2.62547610e-03 -3.41944188e-01 ... -6.59946129e-02\n",
      "     2.14075670e-01  2.18404964e-01]\n",
      "   [-2.43620276e-01  9.95005760e-03  5.26882224e-02 ... -2.22543776e-01\n",
      "     1.71743855e-01 -4.81150206e-03]\n",
      "   [ 2.82130786e-03  5.21088368e-04 -5.06370850e-02 ...  1.23598747e-01\n",
      "    -9.01898667e-02 -2.58086413e-01]]]\n",
      "\n",
      "\n",
      " [[[ 1.94924816e-01  3.38495187e-02 -7.21677206e-03 ... -1.13473967e-01\n",
      "     2.87303478e-02  6.78766742e-02]\n",
      "   [-1.97729677e-01 -4.13105823e-02 -4.31971450e-04 ...  1.84494302e-01\n",
      "    -1.86667949e-01 -7.39383176e-02]\n",
      "   [-1.28780514e-01 -9.07372534e-02  6.90810708e-03 ...  1.41269239e-02\n",
      "     3.21756333e-01  2.40740981e-02]\n",
      "   ...\n",
      "   [-1.59627374e-03 -8.92992690e-03  1.23854324e-01 ...  1.87242054e-04\n",
      "     1.19735457e-01 -3.33650336e-02]\n",
      "   [ 3.30612779e-01  1.87294379e-01  7.49179125e-02 ... -2.49879941e-01\n",
      "    -1.17600210e-01  4.91199195e-02]\n",
      "   [-1.95899859e-01 -1.95380718e-01  6.79195225e-02 ...  4.38252777e-01\n",
      "     1.12858871e-02  4.64277789e-02]]\n",
      "\n",
      "  [[ 1.36589259e-02  1.02708479e-02 -4.13133996e-03 ... -1.03726447e-01\n",
      "     4.55119871e-02 -2.72549279e-02]\n",
      "   [ 1.05953477e-01 -2.48136390e-02  6.01037554e-02 ... -3.25360857e-02\n",
      "    -9.94024277e-02  9.67890024e-04]\n",
      "   [-3.49352419e-01 -2.21630931e-03  1.21791707e-02 ...  2.08576143e-01\n",
      "     7.42298961e-02 -7.20813721e-02]\n",
      "   ...\n",
      "   [-7.12211356e-02  1.21002598e-02  2.48036250e-01 ...  2.23982766e-01\n",
      "     3.89062129e-02 -1.89144447e-01]\n",
      "   [ 1.04240224e-01 -2.75965501e-03  9.15498435e-02 ... -1.98995560e-01\n",
      "     1.12932816e-01 -7.83190057e-02]\n",
      "   [-2.77441651e-01 -7.62819946e-02  6.76504001e-02 ...  6.47811294e-01\n",
      "    -1.51752800e-01 -2.04584450e-02]]\n",
      "\n",
      "  [[ 1.28921300e-01  1.50206052e-02  1.08996727e-01 ... -3.06345612e-01\n",
      "     1.92386489e-02 -1.09747551e-01]\n",
      "   [-1.80957183e-01  1.52300939e-01 -1.48439631e-01 ... -1.79513007e-01\n",
      "    -6.68994384e-04 -1.17112860e-01]\n",
      "   [-6.11315012e-01  1.50389671e-02 -2.24437520e-01 ...  6.01085961e-01\n",
      "     8.88923705e-02 -1.21929675e-01]\n",
      "   ...\n",
      "   [-5.10481559e-03  3.54408547e-02  1.20154500e-01 ...  1.94218755e-01\n",
      "     1.29670501e-01 -3.02845269e-01]\n",
      "   [ 3.83511442e-03  6.77789329e-03  1.81966245e-01 ... -4.80124280e-02\n",
      "    -2.33365800e-02  2.52267383e-02]\n",
      "   [ 6.52493909e-02  5.70870703e-04  2.15067826e-02 ...  2.99304336e-01\n",
      "    -1.80098727e-01 -2.05756262e-01]]]]\n"
     ]
    }
   ],
   "source": [
    "for idx, weights in enumerate(conv_weights):\n",
    "    for w in weights:\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[ 1., -1.,  1., ...,  1.,  1., -1.],\n",
      "        [ 1., -1.,  1., ...,  1., -1., -1.],\n",
      "        [ 1., -1., -1., ..., -1., -1., -1.],\n",
      "        ...,\n",
      "        [ 1.,  1., -1., ..., -1.,  1., -1.],\n",
      "        [ 1.,  1.,  1., ..., -1.,  1., -1.],\n",
      "        [ 1.,  1.,  1., ..., -1., -1.,  1.]]], dtype=float32), array([[[ 1.,  1.,  1., ..., -1.,  1.,  1.],\n",
      "        [ 1., -1., -1., ..., -1.,  1., -1.],\n",
      "        [ 1., -1., -1., ...,  1.,  1., -1.],\n",
      "        ...,\n",
      "        [-1., -1., -1., ...,  1.,  1., -1.],\n",
      "        [-1., -1.,  1., ..., -1.,  1., -1.],\n",
      "        [-1.,  1.,  1., ..., -1., -1.,  1.]]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "QD_weights = []\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, lq.layers.QuantDense):\n",
    "        weights = layer.get_weights()  \n",
    "        QD_weights.append(weights)\n",
    "\n",
    "binarized_weights = []\n",
    "binarized_weight = [np.sign(w) for w in QD_weights]\n",
    "binarized_weights.append(binarized_weight)\n",
    "\n",
    "for w in binarized_weights:\n",
    "    print(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 5ms/step - loss: 0.5302 - accuracy: 0.9654\n",
      "+sequential_1 stats---------------------------------------------------------------------------+\n",
      "| Layer                  Input prec.           Outputs  # 1-bit  # 32-bit  Memory  1-bit MACs |\n",
      "|                              (bit)                        x 1       x 1    (kB)             |\n",
      "+---------------------------------------------------------------------------------------------+\n",
      "| quant_conv2d_2                   1  (-1, 25, 25, 32)      512         0    0.06      320000 |\n",
      "| max_pooling2d_2                  -  (-1, 12, 12, 32)        0         0       0           0 |\n",
      "| batch_normalization_4            -  (-1, 12, 12, 32)        0        64    0.25           0 |\n",
      "| quant_conv2d_3                   1  (-1, 10, 10, 32)     9216         0    1.12      921600 |\n",
      "| max_pooling2d_3                  -    (-1, 5, 5, 32)        0         0       0           0 |\n",
      "| batch_normalization_5            -    (-1, 5, 5, 32)        0        64    0.25           0 |\n",
      "| flatten_1                        -         (-1, 800)        0         0       0           0 |\n",
      "| quant_dense_2                    1         (-1, 128)   102400         0   12.50      102400 |\n",
      "| batch_normalization_6            -         (-1, 128)        0       256    1.00           0 |\n",
      "| quant_dense_3                    1          (-1, 10)     1280         0    0.16        1280 |\n",
      "| batch_normalization_7            -          (-1, 10)        0        20    0.08           0 |\n",
      "| activation_1                     -          (-1, 10)        0         0       0           ? |\n",
      "+---------------------------------------------------------------------------------------------+\n",
      "| Total                                                  113408       404   15.42     1345280 |\n",
      "+---------------------------------------------------------------------------------------------+\n",
      "+sequential_1 summary--------------------------+\n",
      "| Total params                      114 k      |\n",
      "| Trainable params                  113 k      |\n",
      "| Non-trainable params              404        |\n",
      "| Model size                        15.42 KiB  |\n",
      "| Model size (8-bit FP weights)     14.24 KiB  |\n",
      "| Float-32 Equivalent               444.58 KiB |\n",
      "| Compression Ratio of Memory       0.03       |\n",
      "| Number of MACs                    1.35 M     |\n",
      "| Ratio of MACs that are binarized  1.0000     |\n",
      "+----------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Train NN\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "lq.models.summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: batch_normalization_4\n",
      "  Beta (offset): [-0.2879224   0.14712241 -0.3660014  -0.55451655 -0.46084732  0.07310845\n",
      " -0.46811923  0.00877026  0.17918943  0.01968428  0.06558601 -0.21656477\n",
      " -0.22575213  0.20452386  0.05187485 -0.21686895 -0.06975008  0.41787812\n",
      " -0.09844225 -0.22919664 -0.39336184 -0.34981033 -0.09234742  0.05917546\n",
      "  0.23391645 -0.41614062 -0.32683378  0.05042733 -0.3416864  -0.31101266\n",
      " -0.00902344 -0.03541378]\n",
      "Beta Length: 32\n",
      "  Moving Mean: [ 2.9491184   0.36669996  4.717375    0.10981477 -1.6219152   2.5190785\n",
      "  4.7805      0.18862577 -1.7207628   2.161303    1.0617834   0.47486326\n",
      " -1.7039641   2.1541462   0.66630006  3.7714174   0.61636835 -0.74788356\n",
      "  4.423035    5.0047507   6.0805907   7.170473    2.9640634  -2.5170598\n",
      " -2.3864388  -4.591264    1.2148725  -3.690564    2.6914933   6.0606675\n",
      "  2.2720606   5.1316376 ]\n",
      " Moving Mean Length: 32\n",
      "  Moving Variance: [ 4.347042  12.445802   9.779538  14.645649  20.276499  15.462274\n",
      "  9.746536  18.032684  16.484512  10.996992  15.589273  10.396388\n",
      " 15.665088   9.477549  14.645912   7.642922  11.029379  16.270771\n",
      " 13.237978  13.753659  12.261784  17.627148   5.9399624 20.803545\n",
      " 23.570461  29.872442  14.227067  29.080013  13.296489  15.992505\n",
      "  6.7126675 14.552268 ]\n",
      "  Moving Variance Length: 32\n",
      "Layer: batch_normalization_5\n",
      "  Beta (offset): [-0.51303464  0.1258393  -0.24255662 -0.40774062 -0.24911408 -0.22208501\n",
      "  0.0719433  -0.4251291  -0.42774284 -0.46307546 -0.1288512  -0.5788077\n",
      " -0.300955   -0.33685994 -0.6925304  -0.48756683 -0.24871662 -0.17256166\n",
      " -0.45916513 -0.20472491 -0.3769211  -0.33467725 -0.7839839  -0.29339626\n",
      " -0.58540666 -0.06989313 -0.0375315  -0.34958866 -0.09760352 -0.46177053\n",
      " -0.4930538  -0.48033604]\n",
      "Beta Length: 32\n",
      "  Moving Mean: [26.154917  22.681988  10.222062   8.694328  24.395727  32.81635\n",
      " -3.448205  27.475218  10.847296  25.153502  27.222738   1.6502383\n",
      " 19.691748  25.853605  30.277527  24.992239   9.6269655 25.92753\n",
      " 31.07342   21.205923   9.715021  11.39056   12.657908  13.026526\n",
      " 10.569654  26.175203  10.682921  23.352541  34.00586   28.150385\n",
      " 17.934084  17.178926 ]\n",
      " Moving Mean Length: 32\n",
      "  Moving Variance: [ 683.4302   887.68134  605.47345  729.9673   594.68176  794.0283\n",
      " 1089.2083   667.6176   822.82434  863.0768   790.80756  352.60657\n",
      "  772.3269   933.87225  844.13245  674.6204   725.0633   508.45367\n",
      " 1373.134   1111.9574  1012.56055  699.2114   355.12317  953.25586\n",
      "  813.9288   917.4656  1178.0763   996.2944  1343.4465   688.2286\n",
      "  982.82715  618.1525 ]\n",
      "  Moving Variance Length: 32\n",
      "Layer: batch_normalization_6\n",
      "  Beta (offset): [ 4.06253844e-01 -6.09653771e-01  5.79333842e-01 -4.91571635e-01\n",
      "  1.16802745e-01 -2.74137348e-01 -6.94841802e-01 -3.42021257e-01\n",
      " -6.58792853e-01 -2.32978627e-01 -4.03754085e-01 -7.35064566e-01\n",
      "  9.59186703e-02 -4.41284746e-01  1.38055548e-01  4.44414437e-01\n",
      "  7.71608651e-01 -7.05120862e-01 -4.28290009e-01  5.56285441e-01\n",
      " -8.96468535e-02 -4.12090600e-01  3.35154176e-01 -4.57124785e-03\n",
      " -8.98983181e-01  5.85247993e-01  3.91525358e-01  7.04415977e-01\n",
      " -2.61193037e-01 -5.18520057e-01  1.25662848e-01  1.13792084e-01\n",
      "  7.56714046e-01  8.63028914e-02 -2.12502591e-02  7.31787562e-01\n",
      "  4.32980806e-01 -1.90553769e-01  6.57938302e-01  4.11698192e-01\n",
      " -6.40468955e-01 -6.70096934e-01  4.74600792e-01 -4.87375975e-01\n",
      "  6.32683039e-01 -3.47741663e-01  4.92313236e-01  5.23351014e-01\n",
      " -8.86012688e-02  6.93729699e-01 -3.26232016e-01 -2.06199065e-01\n",
      "  7.77262390e-01 -5.05905807e-01 -9.25529972e-02 -2.06929848e-01\n",
      " -4.27841485e-01 -2.38297670e-03  6.23954892e-01 -2.95319140e-01\n",
      "  9.02604878e-01 -8.69289100e-01  3.96528482e-01  3.01697233e-04\n",
      "  3.51756245e-01  2.86044419e-01  6.62116647e-01  5.63996851e-01\n",
      " -2.74492234e-01  5.88654757e-01 -3.99990261e-01  2.29830548e-01\n",
      " -3.36815357e-01  9.17841196e-02  3.80120240e-02 -6.89574063e-01\n",
      " -3.11329570e-02 -6.84556246e-01 -7.81958640e-01 -7.02297628e-01\n",
      " -3.54869753e-01  7.08070695e-01 -2.14638144e-01  3.48659188e-01\n",
      " -1.81367666e-01 -7.23969698e-01 -1.07320093e-01  4.65237886e-01\n",
      " -6.40003502e-01  4.25529569e-01 -5.20203151e-02 -6.95109069e-01\n",
      "  9.59313940e-03 -6.03226840e-01  7.22576439e-01  1.64331049e-01\n",
      " -4.06572446e-02  2.04798222e-01 -7.08459020e-01 -5.94811499e-01\n",
      "  2.90651679e-01 -5.59834003e-01  7.91922867e-01  1.10060290e-01\n",
      " -2.32740700e-01 -6.61371291e-01  3.55946720e-01  5.68089783e-01\n",
      "  4.09667224e-01  6.62568629e-01 -7.07009792e-01 -3.52867335e-01\n",
      "  5.33682227e-01  2.47894779e-01  5.93772233e-01  1.78931981e-01\n",
      " -5.68845987e-01  5.29030152e-02  6.93678379e-01  3.60506117e-01\n",
      " -7.28752613e-01  6.63786352e-01 -3.90382744e-02 -4.09277678e-01\n",
      " -7.67635107e-01 -6.92881763e-01  4.71201867e-01 -1.00552857e-01]\n",
      "Beta Length: 128\n",
      "  Moving Mean: [ 37.810047     1.3815571  -26.322922   -61.60434     34.795395\n",
      "   6.880068    -6.958065     9.744519   -37.50323     20.840958\n",
      " -20.30766    -31.71563     20.108696   -53.54832     13.830498\n",
      "  41.17739     10.562773    -5.990824    -2.2082176   41.06681\n",
      "  26.580345    55.67923     29.732204     9.575714   -33.859825\n",
      "  19.860653   -11.603157    -5.9987144  -25.317156   -16.90436\n",
      "  35.294697     5.854841     7.1353426  -40.034016    36.434795\n",
      "  27.768574   -13.730299   -32.581303   -39.40966     -3.518802\n",
      " -24.450144   -24.18216     24.924704   -45.510735     2.3331523\n",
      "  13.936656   -75.96757     34.67638     21.513344   -39.412548\n",
      "  37.91285      7.226487   -16.558023    -9.139202    17.856401\n",
      " -25.201073    16.32788     -2.575503   -50.708027    -2.9130652\n",
      "  -5.459391    11.63451    -36.628933    34.062042   -21.913792\n",
      " -14.027179   -57.495113   -38.25075     29.036894   -42.5579\n",
      " -12.6473465   -0.10539927 -47.883064    15.607839     1.4834033\n",
      "  45.57304    -23.081245     8.844112   -35.885113    36.525978\n",
      "  28.949245    20.447079     1.1826147  -17.426693    25.952976\n",
      "  12.748583   -17.412474    28.265226    -9.689578    -5.1406813\n",
      "  65.49298    -13.519052     1.6491153   -8.643395    -7.8132057\n",
      "  28.947367   -38.454838    -4.5991387   21.73363     28.775694\n",
      "  16.365389     0.07833562  -0.20237613  26.073198     5.417079\n",
      "   1.9952351  -24.6634     -28.908434   -11.8990345   44.887814\n",
      " -55.10354    -45.433067    11.464279   -53.028152    -9.922851\n",
      "  -5.05548      1.7345141  -69.99605    -11.900254   -36.68337\n",
      "  12.954965    14.123557    12.292212   -12.725163     6.5068054\n",
      "  -9.52761    -53.42143     39.139225  ]\n",
      " Moving Mean Length: 128\n",
      "  Moving Variance: [2238.004  1868.327  3580.1055 2701.4866 4828.9287 2747.556  2366.1162\n",
      " 2117.522  2781.282  2906.1255 3012.     2609.5215 3022.873  3425.2134\n",
      " 2849.7068 2826.9575 4070.3894 2861.7534 2485.9065 2292.797  3248.6646\n",
      " 2536.2131 3026.7957 3169.0085 3854.108  2847.9077 3557.3906 2400.1362\n",
      " 3477.2073 2942.2627 2607.139  2328.9895 5506.051  2284.9668 2242.7197\n",
      " 3302.0332 2224.5876 2620.33   3563.5151 2924.3877 3644.1409 3605.901\n",
      " 3337.8987 3068.3462 3722.679  2800.7334 2137.9963 2753.8647 1861.8007\n",
      " 3664.9927 2402.052  4339.843  3318.399  2494.1821 2432.961  4250.6978\n",
      " 3000.928  2626.0847 3323.3528 2257.5542 3293.984  3044.6602 2721.5796\n",
      " 2087.8933 3502.998  2795.299  3148.984  2324.499  2652.3943 2865.4001\n",
      " 3588.5476 3255.0159 2389.2168 2833.44   2394.6743 2315.831  3083.361\n",
      " 3453.348  4280.2446 3359.9294 2738.7617 2521.4958 2938.7473 2772.815\n",
      " 2482.7004 3695.0083 3377.118  3321.063  2204.6436 2957.362  4097.221\n",
      " 3484.7964 3055.179  2819.054  3186.533  2515.179  2462.7058 2253.7842\n",
      " 2807.3562 3538.5366 3241.8708 4998.467  2947.6243 1991.9384 3311.6348\n",
      " 3532.2769 3443.5557 3175.6719 2408.581  2380.3235 2616.5356 2594.5676\n",
      " 3040.4597 2694.491  3128.5986 2440.4917 3017.023  3257.542  1949.6858\n",
      " 2076.3772 4207.593  4296.309  2315.9922 3120.9795 3414.0334 2904.5842\n",
      " 3316.855  3935.685 ]\n",
      "  Moving Variance Length: 128\n",
      "Layer: batch_normalization_7\n",
      "  Beta (offset): [-0.03242603  0.14275455 -0.00576703  0.0263354  -0.02614635 -0.14818974\n",
      " -0.02780115  0.0695119  -0.02072958  0.00118044]\n",
      "Beta Length: 10\n",
      "  Moving Mean: [ 2.3267264   3.7937016   2.4891841   6.2153974   1.1779317   1.6034378\n",
      "  0.23226152 -1.5119069  14.389101    1.652033  ]\n",
      " Moving Mean Length: 10\n",
      "  Moving Variance: [736.1596  876.0502  639.31525 682.6485  672.9578  627.44196 732.91284\n",
      " 635.8585  652.64703 599.5883 ]\n",
      "  Moving Variance Length: 10\n"
     ]
    }
   ],
   "source": [
    "# # Extract weights\n",
    "# with lq.context.quantized_scope(True):\n",
    "#     weights = model.layers[3].get_weights()\n",
    "#     print(weights)\n",
    "\n",
    "#     if len(weights) > 0:\n",
    "#         weight_array = weights[0] \n",
    "#         print(\"Weights shape:\", weight_array.shape)\n",
    "#     else:\n",
    "#         print(\"No weights found in this layer.\")\n",
    "\n",
    "# print(weights[0].shape)\n",
    "# rows, cols, _, output_channels = weights[0].shape\n",
    "# print(rows, cols, output_channels)\n",
    "# for col in range(cols):\n",
    "#     for row in range(rows):\n",
    "#         for output_channel in range(output_channels):\n",
    "#             print(row, col, output_channel, weights[0][row][col][0][output_channel])\n",
    "\n",
    "# for layer in model.layers:\n",
    "#     if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "#         beta, moving_mean, moving_variance = layer.get_weights()\n",
    "#         print(f\"Layer: {layer.name}\")\n",
    "#         print(f\"  Beta (offset): {beta}\")\n",
    "#         print(f\"Beta Length: {len(beta)}\")\n",
    "#         print(f\"  Moving Mean: {moving_mean}\")\n",
    "#         print(f\" Moving Mean Length: {len(moving_mean)}\")\n",
    "#         print(f\"  Moving Variance: {moving_variance}\")\n",
    "#         print(f\"  Moving Variance Length: {len(moving_variance)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: batch_normalization_4, Input shape: (None, 12, 12, 32)\n",
      "Layer: batch_normalization_5, Input shape: (None, 5, 5, 32)\n",
      "Layer: batch_normalization_6, Input shape: (None, 128)\n",
      "Layer: batch_normalization_7, Input shape: (None, 10)\n"
     ]
    }
   ],
   "source": [
    "input_shape_callback = InputShapeCallback()\n",
    "\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "        input_shape = layer.input_shape\n",
    "        print(f\"Layer: {layer.name}, Input shape: {input_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"gen_hdl\"):\n",
    "    os.mkdir(\"gen_hdl\")\n",
    "\n",
    "# Extract weights\n",
    "betas = []\n",
    "moving_means = []\n",
    "moving_variances = []\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "        beta, moving_mean, moving_variance = layer.get_weights()\n",
    "        betas.append(beta)\n",
    "        moving_means.append(moving_mean)\n",
    "        moving_variances.append(moving_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_bn(beta, moving_mean, moving_variance, num: int):\n",
    "\n",
    "    # thresholds = np.zeros(len(beta))\n",
    "    compare = \"\"\n",
    "    for output_neuron in range(len(beta)):\n",
    "        # print(len(beta))\n",
    "        threshold = moving_mean[output_neuron] - beta[output_neuron] * np.sqrt(moving_variance[output_neuron])\n",
    "        compare += f\"   assign o_data[{output_neuron}] = i_data[{output_neuron}] > {threshold} ? 1 : 0;\\n\"\n",
    "\n",
    "    output_hdl = templates.BN_TEMPLATE \\\n",
    "        .replace(\"%DIM_DATA%\", str(len(beta))) \\\n",
    "        .replace(\"%LAYER_NUM%\", str(num)) \\\n",
    "        .replace(\"%COMPARE%\", compare)\n",
    "        \n",
    "    with open(f\"gen_hdl/bn_layer_{num}.v\", \"w\") as f:\n",
    "        f.write(output_hdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 1, 32)\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(betas)):\n",
    "    parse_bn(betas[n], moving_means[n], moving_variances[n], n)\n",
    "\n",
    "weights = model.layers[0].get_weights()\n",
    "w = weights[0].reshape(16, 1, 32)\n",
    "print(w.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "938/938 [==============================] - 5s 4ms/step - loss: 0.8683 - accuracy: 0.8200\n",
      "Epoch 2/6\n",
      "938/938 [==============================] - 3s 4ms/step - loss: 0.6958 - accuracy: 0.8759\n",
      "Epoch 3/6\n",
      "938/938 [==============================] - 3s 4ms/step - loss: 0.6754 - accuracy: 0.8816\n",
      "Epoch 4/6\n",
      "938/938 [==============================] - 3s 4ms/step - loss: 0.6720 - accuracy: 0.8832\n",
      "Epoch 5/6\n",
      "938/938 [==============================] - 3s 4ms/step - loss: 0.6668 - accuracy: 0.8844\n",
      "Epoch 6/6\n",
      "938/938 [==============================] - 3s 4ms/step - loss: 0.6599 - accuracy: 0.8848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2ae8cd54970>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PrunableQuantDense(lq.layers.QuantDense, sparsity.PrunableLayer):\n",
    "    def get_prunable_weights(self):\n",
    "        return [self.kernel]\n",
    "\n",
    "pruning_params = {\n",
    "    'pruning_schedule': sparsity.PolynomialDecay(\n",
    "        initial_sparsity=0.0,\n",
    "        final_sparsity=0.5,\n",
    "        begin_step=0,\n",
    "        end_step=len(train_images) // 64 * 6\n",
    "    )\n",
    "}\n",
    "\n",
    "kwargs = dict(input_quantizer=\"ste_sign\",\n",
    "              kernel_quantizer=\"ste_sign\",\n",
    "              kernel_constraint=\"weight_clip\")\n",
    "\n",
    "model_new = tf.keras.models.Sequential()\n",
    "\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "model_new.add(tf.keras.layers.Flatten(input_shape=input_shape))\n",
    "\n",
    "# 128 neurons\n",
    "model_new.add(sparsity.prune_low_magnitude(PrunableQuantDense(128, use_bias=False, **kwargs), **pruning_params))\n",
    "model_new.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "# 128 neurons\n",
    "model_new.add(sparsity.prune_low_magnitude(PrunableQuantDense(128, use_bias=False, **kwargs), **pruning_params))\n",
    "model_new.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "# 64 neurons\n",
    "model_new.add(sparsity.prune_low_magnitude(PrunableQuantDense(64, use_bias=False, **kwargs), **pruning_params))\n",
    "model_new.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "# 10 neurons\n",
    "model_new.add(sparsity.prune_low_magnitude(PrunableQuantDense(10, use_bias=False, **kwargs), **pruning_params))\n",
    "model_new.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "model_new.add(tf.keras.layers.Activation(\"softmax\"))\n",
    "\n",
    "model_new.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "model_new.fit(train_images, train_labels,\n",
    "              batch_size=64, epochs=6,\n",
    "              callbacks=[sparsity.UpdatePruningStep()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 1ms/step - loss: 0.5818 - accuracy: 0.9023\n",
      "Stripped Pruned Model Test Loss: 0.5818113088607788\n",
      "Stripped Pruned Model Test Accuracy: 0.9023000001907349\n",
      "+sequential_9 stats----------------------------------------------------------------------+\n",
      "| Layer                    Input prec.    Outputs  # 1-bit  # 32-bit  Memory  1-bit MACs |\n",
      "|                                (bit)                 x 1       x 1    (kB)             |\n",
      "+----------------------------------------------------------------------------------------+\n",
      "| flatten_9                          -  (-1, 784)        0         0       0           0 |\n",
      "| prunable_quant_dense_16            1  (-1, 128)   100352         0   12.25      100352 |\n",
      "| batch_normalization_36             -  (-1, 128)        0       256    1.00           0 |\n",
      "| prunable_quant_dense_17            1  (-1, 128)    16384         0    2.00       16384 |\n",
      "| batch_normalization_37             -  (-1, 128)        0       256    1.00           0 |\n",
      "| prunable_quant_dense_18            1   (-1, 64)     8192         0    1.00        8192 |\n",
      "| batch_normalization_38             -   (-1, 64)        0       128    0.50           0 |\n",
      "| prunable_quant_dense_19            1   (-1, 10)      640         0    0.08         640 |\n",
      "| batch_normalization_39             -   (-1, 10)        0        20    0.08           0 |\n",
      "| activation_15                      -   (-1, 10)        0         0       0           ? |\n",
      "+----------------------------------------------------------------------------------------+\n",
      "| Total                                             125568       660   17.91      125568 |\n",
      "+----------------------------------------------------------------------------------------+\n",
      "+sequential_9 summary--------------------------+\n",
      "| Total params                      126 k      |\n",
      "| Trainable params                  126 k      |\n",
      "| Non-trainable params              660        |\n",
      "| Model size                        17.91 KiB  |\n",
      "| Model size (8-bit FP weights)     15.97 KiB  |\n",
      "| Float-32 Equivalent               493.08 KiB |\n",
      "| Compression Ratio of Memory       0.04       |\n",
      "| Number of MACs                    126 k      |\n",
      "| Ratio of MACs that are binarized  1.0000     |\n",
      "+----------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final_model = tfmot.sparsity.keras.strip_pruning(model_new)\n",
    "final_model.compile(optimizer='adam',\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "\n",
    "final_test_loss, final_test_accuracy = final_model.evaluate(test_images, test_labels)\n",
    "print(f'Stripped Pruned Model Test Loss: {final_test_loss}')\n",
    "print(f'Stripped Pruned Model Test Accuracy: {final_test_accuracy}')\n",
    "lq.models.summary(final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.8180 - accuracy: 0.8333\n",
      "Epoch 2/6\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.6024 - accuracy: 0.9033\n",
      "Epoch 3/6\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.5620 - accuracy: 0.9161\n",
      "Epoch 4/6\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.5341 - accuracy: 0.9247\n",
      "Epoch 5/6\n",
      "938/938 [==============================] - 3s 4ms/step - loss: 0.5259 - accuracy: 0.9294\n",
      "Epoch 6/6\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.5145 - accuracy: 0.9318\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_5 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " quant_dense_12 (QuantDense  (None, 128)               100352    \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization_20 (Ba  (None, 128)               384       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " quant_dense_13 (QuantDense  (None, 128)               16384     \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization_21 (Ba  (None, 128)               384       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " quant_dense_14 (QuantDense  (None, 64)                8192      \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization_22 (Ba  (None, 64)                192       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " quant_dense_15 (QuantDense  (None, 10)                640       \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization_23 (Ba  (None, 10)                30        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 126558 (494.37 KB)\n",
      "Trainable params: 125898 (491.79 KB)\n",
      "Non-trainable params: 660 (2.58 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_fc = tf.keras.models.Sequential()\n",
    "\n",
    "\n",
    "input_shape = (28, 28, 1)  \n",
    "\n",
    "\n",
    "model_fc.add(tf.keras.layers.Flatten(input_shape=input_shape))\n",
    "\n",
    "\n",
    "model_fc.add(lq.layers.QuantDense(128, use_bias=False, **kwargs))\n",
    "model_fc.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "model_fc.add(lq.layers.QuantDense(128, use_bias=False, **kwargs))\n",
    "model_fc.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "model_fc.add(lq.layers.QuantDense(64, use_bias=False, **kwargs))\n",
    "model_fc.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "\n",
    "model_fc.add(lq.layers.QuantDense(10, use_bias=False, **kwargs))\n",
    "model_fc.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "model_fc.add(tf.keras.layers.Activation(\"softmax\"))\n",
    "\n",
    "model_fc.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_fc.fit(train_images, train_labels, batch_size=64, epochs=6)\n",
    "\n",
    "model_fc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 1ms/step - loss: 0.4545 - accuracy: 0.9431\n",
      "+sequential_5 stats---------------------------------------------------------------------+\n",
      "| Layer                   Input prec.    Outputs  # 1-bit  # 32-bit  Memory  1-bit MACs |\n",
      "|                               (bit)                 x 1       x 1    (kB)             |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "| flatten_5                         -  (-1, 784)        0         0       0           0 |\n",
      "| quant_dense_12                    1  (-1, 128)   100352         0   12.25      100352 |\n",
      "| batch_normalization_20            -  (-1, 128)        0       256    1.00           0 |\n",
      "| quant_dense_13                    1  (-1, 128)    16384         0    2.00       16384 |\n",
      "| batch_normalization_21            -  (-1, 128)        0       256    1.00           0 |\n",
      "| quant_dense_14                    1   (-1, 64)     8192         0    1.00        8192 |\n",
      "| batch_normalization_22            -   (-1, 64)        0       128    0.50           0 |\n",
      "| quant_dense_15                    1   (-1, 10)      640         0    0.08         640 |\n",
      "| batch_normalization_23            -   (-1, 10)        0        20    0.08           0 |\n",
      "| activation_11                     -   (-1, 10)        0         0       0           ? |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Total                                            125568       660   17.91      125568 |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "+sequential_5 summary--------------------------+\n",
      "| Total params                      126 k      |\n",
      "| Trainable params                  126 k      |\n",
      "| Non-trainable params              660        |\n",
      "| Model size                        17.91 KiB  |\n",
      "| Model size (8-bit FP weights)     15.97 KiB  |\n",
      "| Float-32 Equivalent               493.08 KiB |\n",
      "| Compression Ratio of Memory       0.04       |\n",
      "| Number of MACs                    126 k      |\n",
      "| Ratio of MACs that are binarized  1.0000     |\n",
      "+----------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model_fc.evaluate(test_images, test_labels)\n",
    "lq.models.summary(model_fc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
